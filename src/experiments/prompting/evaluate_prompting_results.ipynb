{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment resources related to the QUITE corpus (EMNLP 2024).\n",
    "\n",
    "Copyright (c) 2024 Robert Bosch GmbH\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Affero General Public License as published\n",
    "by the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Affero General Public License for more details.\n",
    "You should have received a copy of the GNU Affero General Public License\n",
    "along with this program.  If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Split\n",
    "import re\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "\n",
    "from src.constants import QUITE_Config, FLOAT_PERCENTAGE_PATTERN, PERCENTAGE_PATTERN, INVALID_INDICATION_FLAG, PROJECT_ROOT\n",
    "from src.experiments.src.numeric_evaluator import NumericEvaluator\n",
    "from src.utils.quite_dataset_loaders import quite_dataset_mappings\n",
    "\n",
    "ne: NumericEvaluator = NumericEvaluator()\n",
    "dataset: Dataset = quite_dataset_mappings[Split.TEST][QUITE_Config.EVIDENCE_QUERY_PAIRS.value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(input_paths: dict[str, dict[str, str]], dataset: Dataset) -> dict[str, dict[str, dict]] | dict[str, dict[str, dict]]:\n",
    "\n",
    "    result_dict: dict[str, dict[str, dict]] = {}\n",
    "    preds_per_network: dict[str, dict[str, dict]] = {}\n",
    "\n",
    "    for prompt in input_paths.keys():\n",
    "        result_dict[prompt] = {}\n",
    "        preds_per_network[prompt] = {}\n",
    "        for model, path in input_paths[prompt].items():\n",
    "            if path == \"\":\n",
    "                continue\n",
    "            non_error_instances: list[str] = []\n",
    "            predicted_probs: list[float] = []\n",
    "            ground_truth_probs: list[float] = []\n",
    "            reasoning_types: list[list[str]] = []\n",
    "            preds_per_network[prompt][model] = {}\n",
    "            for file in listdir(path):\n",
    "                if not \"hepar2_1\" in file:\n",
    "                    network_name: str = file.split(\"_\")[0]\n",
    "                    qe_id: int = int(file.split(\"_\")[1][:-4])\n",
    "                else:\n",
    "                    network_name: str = \"hepar2_1\"\n",
    "                    qe_id: int = int(file[len(network_name)+1:-4])\n",
    "                with open(join(path, file)) as f:\n",
    "                    content: str = f.read()\n",
    "\n",
    "                non_error_instances.append(file)\n",
    "\n",
    "\n",
    "                subset = dataset.filter(lambda x: x[\"qe_id\"] == qe_id);\n",
    "\n",
    "                answer: float = float(subset[-1][\"answer\"])\n",
    "                ground_truth_probs.append(answer)\n",
    "                reasoning_types.append(subset[-1][\"reasoning_types\"])\n",
    "\n",
    "                assert subset[-1][\"input\"] in content\n",
    "\n",
    "                final_output: str = content[-350:]\n",
    "\n",
    "                # First look for percentage\n",
    "                percentages: list[str] = re.findall(PERCENTAGE_PATTERN, final_output)\n",
    "                if len(percentages) > 0:\n",
    "                    normalized_float_value: float = float(percentages[-1].rstrip(\"%\")) / 100\n",
    "                    predicted_probs.append(normalized_float_value)\n",
    "                elif len(re.findall(FLOAT_PERCENTAGE_PATTERN, final_output)) > 0: # Find last float number\n",
    "                    normalized_float_value: float = float(re.findall(FLOAT_PERCENTAGE_PATTERN, final_output)[-1])\n",
    "                    predicted_probs.append(normalized_float_value)\n",
    "                else: # Insert default value\n",
    "                    predicted_probs.append(INVALID_INDICATION_FLAG)\n",
    "                    non_error_instances = non_error_instances[:-1]\n",
    "                \n",
    "                if network_name not in preds_per_network[prompt][model]:\n",
    "                    preds_per_network[prompt][model][network_name] = {\"predicted\": [], \"gt\": [], \"rt\": []}\n",
    "                preds_per_network[prompt][model][network_name][\"predicted\"].append(normalized_float_value)\n",
    "                preds_per_network[prompt][model][network_name][\"gt\"].append(answer)\n",
    "                preds_per_network[prompt][model][network_name][\"rt\"].append(subset[-1][\"reasoning_types\"])\n",
    "\n",
    "            if model == \"gpt-4-turbo\" and prompt == \"causalcot\":\n",
    "                for network_name in preds_per_network[prompt][model].keys():\n",
    "                    network_results = ne.get_metrics(predicted_probs=preds_per_network[prompt][model][network_name][\"predicted\"], true_probs=preds_per_network[prompt][model][network_name][\"gt\"], reasoning_types=preds_per_network[prompt][model][network_name][\"rt\"])\n",
    "                    preds_per_network[prompt][model][network_name][\"accuracy\"] = network_results[\"accuracy\"]\n",
    "                    preds_per_network[prompt][model][network_name][\"wrong\"] = network_results[\"wrong\"]\n",
    "                    preds_per_network[prompt][model][network_name][\"error\"] = network_results[\"error\"]\n",
    "                print(preds_per_network[prompt][model])\n",
    "\n",
    "            result_dict[prompt][model] = ne.get_metrics(predicted_probs=predicted_probs, true_probs=ground_truth_probs, reasoning_types=reasoning_types)\n",
    "\n",
    "    return result_dict, preds_per_network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_input_paths: dict[str, dict[str, str]] = {\n",
    "    \"zero_shot\": {\n",
    "        \"llama-3-8b\": join(PROJECT_ROOT, \"paper_results/prompting/zero-shot/numeric/llama-3-8b\"),\n",
    "        \"mixtral-8x7b\": join(PROJECT_ROOT, \"paper_results/prompting/zero-shot/numeric/mixtral-8x7b\"),\n",
    "        \"gpt-4-turbo\": join(PROJECT_ROOT, \"paper_results/prompting/zero-shot/numeric/gpt4-turbo\"),\n",
    "    },\n",
    "    \"causalcot\": {\n",
    "        \"llama-3-8b\": join(PROJECT_ROOT, \"paper_results/prompting/causalcot/numeric/llama-3-8b\"),\n",
    "        \"mixtral-8x7b\": join(PROJECT_ROOT, \"paper_results/prompting/causalcot/numeric/mixtral-8x7b\"),\n",
    "        \"gpt-4-turbo\": join(PROJECT_ROOT, \"paper_results/prompting/causalcot/numeric/gpt4-turbo\"),\n",
    "    },\n",
    "    \"qe_only\": {\n",
    "        \"gpt-4-turbo\": join(PROJECT_ROOT, \"paper_results/prompting/qe-only/gpt4-turbo\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "result_dict, preds_per_network = calculate_metrics(input_paths=numeric_input_paths, dataset=dataset)\n",
    "\n",
    "for k, v in result_dict.items():\n",
    "    for k2, v2 in v.items():\n",
    "        print(k, \", \", k2, \": \", v2, \"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wep_input_paths: dict[str, dict[str, str]] = {\n",
    "    \"zero_shot\": {\n",
    "        \"llama-3-8b\": join(PROJECT_ROOT, \"paper_results/prompting/zero-shot/wep-based-premises/llama-3-8b\"),\n",
    "        \"mixtral-8x7b\": join(PROJECT_ROOT, \"paper_results/prompting/zero-shot/wep-based-premises/mixtral-8x7b\"),\n",
    "        \"gpt-4-turbo\": join(PROJECT_ROOT, \"paper_results/prompting/zero-shot/wep-based-premises/gpt4-turbo\"),\n",
    "    },\n",
    "    \"causalcot\": {\n",
    "        \"llama-3-8b\": join(PROJECT_ROOT, \"paper_results/prompting/causalcot/wep-based-premises/llama-3-8b\"),\n",
    "        \"mixtral-8x7b\": join(PROJECT_ROOT, \"paper_results/prompting/causalcot/wep-based-premises/mixtral-8x7b\"),\n",
    "        \"gpt-4-turbo\": join(PROJECT_ROOT, \"paper_results/prompting/causalcot/wep-based-premises/gpt4-turbo\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "result_dict, preds_per_network = calculate_metrics(input_paths=wep_input_paths, dataset=dataset)\n",
    "\n",
    "for k, v in result_dict.items():\n",
    "    for k2, v2 in v.items():\n",
    "        print(k, \", \", k2, \": \", v2, \"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
